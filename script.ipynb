{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(39790) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (4.44.2)\n",
      "Requirement already satisfied: torch in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: accelerate in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (0.33.0)\n",
      "Requirement already satisfied: huggingface_hub in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (0.24.6)\n",
      "Requirement already satisfied: torchvision in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (0.19.0)\n",
      "Requirement already satisfied: torchaudio in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: psutil in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch accelerate huggingface_hub torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/umarcheema/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Authenticate with your Hugging Face token\n",
    "token = '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'\n",
    "login(token=token)  # Replace with your actual token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\"Routine maintenance\", \"Repair\", \"Oil Change\", \"Car wash\", \"Car sale\"]\n",
    "outcomes = [\"Led to sale\", \"Could not lead to a sale\"]\n",
    "languages = [\"English\", \"Spanish\", \"French\"]\n",
    "feedback_questions = [\"Happy with AI?\", \"Happy with Human?\", \"Will you use the service again?\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/umarcheema/miniconda3/envs/data-generation/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Generate a transcript for a 5-minute call on routine maintenance. A call to #tbl.ap.au then gets logged into a system on your phone by calling 931-933-4063 over the phone.\\n\\nThis is why we always put the phone line directly beneath our name. So, don't leave our phone number for the public to use. If it runs into a big spike, it probably isn't our fault. What is your problem? Call us on our free number 1300.2033 (9.30am – 4.30pm Monday to Friday). Or get in touch anonymously on Twitter @yourfederalswillcall.\\n\\nFamilies can call +61 3 8325 4000 and we won't keep a secret to call us between the hours of 10pm and 4am. #TBL #TBD #WAP — Department of Finance (@DFFinMinistry) May 25, 2017\\n\\nCall us at 931-933-4063 on Monday to Friday. You'll get the audio file, and your number is kept anonymous. Email us at btlswee@df.gov.au or call us anonymously on Twitter @yourfederalswillcall and tweet us your problems.\\n\\nRead more here.\\n\\n#bipartisan coalition government\\n\\nUpdated: October 28 2017.\\n\\nPosted: Thursday, October 28 2017.\\n\\n12 September 2017\"}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "\n",
    "model_id = \"gpt2-medium\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    device=0 if device.type == 'mps' else -1,  # MPS is treated like CUDA\n",
    ")\n",
    "\n",
    "\n",
    "# Generate text\n",
    "output = text_generation_pipeline(\"Generate a transcript for a 5-minute call on routine maintenance.\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call data generation complete.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "\n",
    "# Initialize GPT-2 model and tokenizer\n",
    "model_id = \"gpt2-medium\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    device=0  # Use 'mps' if on a Mac with M1 chip, otherwise -1 for CPU\n",
    ")\n",
    "\n",
    "# Function to generate random call data\n",
    "def generate_call_data(call_id):\n",
    "    topics = [\"Oil Change\", \"Routine maintenance\", \"Car sale\", \"Car wash\", \"Repair\"]\n",
    "    outcomes = [\"Led to sale\", \"Could not lead to a sale\"]\n",
    "    languages = [\"English\", \"Spanish\", \"French\", \"German\"]\n",
    "    locations = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\"]\n",
    "\n",
    "    call_topic = random.choice(topics)\n",
    "    handled_by_agent = random.choice([\"AI\", \"Human\"])\n",
    "    redirected_to_agent = \"Human\" if handled_by_agent == \"AI\" else \"None\"\n",
    "    outcome = random.choice(outcomes)\n",
    "    length = random.randint(300, 600)  # Call length in seconds (5-10 minutes)\n",
    "    language = random.choice(languages)\n",
    "    location = random.choice(locations)\n",
    "    caller_id = f\"caller-{call_id:03d}\"\n",
    "    agent_id = f\"agent-{call_id:03d}\"\n",
    "    model_version = random.choice([\"v1.0\", \"v1.1\", \"v2.0\"])\n",
    "\n",
    "    # Generate a call transcript using GPT-2\n",
    "    prompt = f\"Generate a transcript for a {length//60}-minute call on {call_topic}.\"\n",
    "    transcript = text_generation_pipeline(prompt)[0][\"generated_text\"]\n",
    "\n",
    "    return {\n",
    "        \"CallID\": f\"call-{call_id:03d}\",\n",
    "        \"Received\": f\"2024-08-24T{8 + call_id % 12:02d}:00:00Z\",\n",
    "        \"Accepted\": True,\n",
    "        \"HandledByAgent\": handled_by_agent,\n",
    "        \"RedirectedToAgent\": redirected_to_agent,\n",
    "        \"Length\": length,\n",
    "        \"Outcome\": outcome,\n",
    "        \"Topic\": call_topic,\n",
    "        \"CallTranscript\": transcript,\n",
    "        \"Feedback\": {\n",
    "            \"HappyWithAI\": handled_by_agent == \"AI\",\n",
    "            \"HappyWithHuman\": redirected_to_agent == \"Human\",\n",
    "            \"WillUseServiceAgain\": random.choice([True, False]),\n",
    "            \"FeedbackTime\": f\"2024-08-24T{8 + call_id % 12:02d}:10:00Z\"\n",
    "        },\n",
    "        \"Caller\": {\n",
    "            \"CallerID\": caller_id,\n",
    "            \"Name\": f\"Name-{call_id:03d}\",\n",
    "            \"PhoneNumber\": f\"+{random.randint(1000000000, 9999999999)}\",\n",
    "            \"Location\": location,\n",
    "            \"IsPreviousCustomer\": random.choice([True, False]),\n",
    "            \"TimeOfCall\": f\"2024-08-24T{8 + call_id % 12:02d}:00:00Z\",\n",
    "            \"Language\": language,\n",
    "            \"CallHistory\": [f\"call-{call_id:03d}\"]\n",
    "        },\n",
    "        \"Agent\": {\n",
    "            \"AgentID\": agent_id,\n",
    "            \"AgentType\": handled_by_agent,\n",
    "            \"Model\": model_version if handled_by_agent == \"AI\" else \"\",\n",
    "            \"Performance\": \"High\" if handled_by_agent == \"AI\" else \"N/A\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Generate data for 100 calls\n",
    "call_data = [generate_call_data(i + 1) for i in range(100)]\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(\"generated_call_data.json\", \"w\") as f:\n",
    "    json.dump(call_data, f, indent=4)\n",
    "\n",
    "print(\"Call data generation complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
